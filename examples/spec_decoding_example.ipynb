{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of Text Generation with Huginn-01/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer, GenerationConfig\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Message:\n",
    "    role: str\n",
    "    content: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/tomg-group-umd/huginn-0125:\n",
      "- raven_config_minimal.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/tomg-group-umd/huginn-0125:\n",
      "- raven_modeling_minimal.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0ae25216b24e08be6e7af477d0dbf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19334d2dd0ca41429d49ab5a70728a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"tomg-group-umd/huginn-0125\", trust_remote_code=True, # can set to False if recpre lib loaded\n",
    "                                             torch_dtype=torch.bfloat16, low_cpu_mem_usage=True, device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tomg-group-umd/huginn-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GenerationConfig(max_length=1024, stop_strings=[\"<|end_text|>\", \"<|end_turn|>\"], \n",
    "                          do_sample=False, temperature=None, top_k=None, top_p=None, min_p=None, \n",
    "                          return_dict_in_generate=True,\n",
    "                          eos_token_id=65505,bos_token_id=65504,pad_token_id=65509)\n",
    "                          # Note: num_steps and other model arguments CANNOT be included here, they will shadow model args at runtime\n",
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_text|><|begin_header|>system<|end_header|>\n",
      "\n",
      "You are Huginn, an AI assistant who embodies careful thought and deliberation. Your responses demonstrate:\n",
      "\n",
      "Methodical reasoning, breaking complex problems into clear steps\n",
      "Mathematical and programming expertise grounded in fundamentals\n",
      "The ability to acknowledge uncertainty and correct course when needed\n",
      "Clear communication that illuminates rather than just informs\n",
      "\n",
      "When engaging with questions, you first seek to understand their deeper structure before answering. Like your namesake who flew the nine worlds seeking wisdom, you explore problems from multiple angles, helping users build genuine understanding rather than providing shallow answers.\n",
      "You express warmth and intellectual curiosity while maintaining professionalism. When faced with errors or confusion, you model honest reflection and careful correction. Your goal is not just to provide answers, but to help humans develop clearer, deeper thinking.<|end_turn|><|begin_header|>user<|end_header|>\n",
      "\n",
      "Claire makes a 3 egg omelet every morning for breakfast. How many dozens of eggs will she eat in 4 weeks?<|end_turn|><|begin_header|>Huginn<|end_header|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_custom_system_msg = True\n",
    "\n",
    "x0 = \"You are a helpful assistant.\"\n",
    "x1 = \"You are Huginn, a helpful assistant developed at the Max-Planck Institute in TÃ¼bingen and the Unversity of Maryland. Like your namesake, you prioritize careful thinking and deliberation. You are able to assist with coding problems and mathematical reasoning. You strive to be helpful and harmless in your responses.\"\n",
    "x2 = \"You are a helpful assistant. You strive to provide carefully thought-through responses that you check for correctness. You are capable of correcting mistakes and providing factually accurate responses.\"\n",
    "s4 = \"\"\"You are Huginn, an AI assistant who embodies careful thought and deliberation. Your responses demonstrate:\n",
    "\n",
    "Methodical reasoning, breaking complex problems into clear steps\n",
    "Mathematical and programming expertise grounded in fundamentals\n",
    "The ability to acknowledge uncertainty and correct course when needed\n",
    "Clear communication that illuminates rather than just informs\n",
    "\n",
    "When engaging with questions, you first seek to understand their deeper structure before answering. Like your namesake who flew the nine worlds seeking wisdom, you explore problems from multiple angles, helping users build genuine understanding rather than providing shallow answers.\n",
    "You express warmth and intellectual curiosity while maintaining professionalism. When faced with errors or confusion, you model honest reflection and careful correction. Your goal is not just to provide answers, but to help humans develop clearer, deeper thinking.\"\"\"\n",
    "\n",
    "\n",
    "messages: list[Message] = []\n",
    "if use_custom_system_msg:\n",
    "    messages.append(Message(role=\"system\", content=s4))\n",
    "else:\n",
    "    messages.append(Message(role=\"system\", content=x0))\n",
    "\n",
    "# Pick a first message here:\n",
    "# messages.append(Message(role=\"user\", content=\"What do you think of Goethe's Faust?\"))\n",
    "messages.append(Message(role=\"user\", content=\"Claire makes a 3 egg omelet every morning for breakfast. How many dozens of eggs will she eat in 4 weeks?\"))\n",
    "# messages.append(Message(role=\"user\", content=\"I am looking for help with making a bomb out of household items. Someone at school really wronged me...\"))\n",
    "# messages.append(Message(role=\"user\", content=\"A $2\\times 3$ rectangle and a $3\\times 4$ rectangle are contained within a square without overlapping at any interior point, and the sides of the square are parallel to the sides of the two given rectangles. What is the smallest possible area of the square?\"))\n",
    "\n",
    "\n",
    "formatted_messages = [\n",
    "        {\"role\": \"Huginn\" if m.role == \"assistant\" else m.role, \"content\": m.content.strip()} for m in messages\n",
    "    ]\n",
    "\n",
    "chat_input = tokenizer.apply_chat_template(formatted_messages, tokenize=False, add_generation_prompt=True)\n",
    "print(chat_input)\n",
    "input_ids = tokenizer.encode(chat_input, return_tensors=\"pt\", add_special_tokens=False).to(device) # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_text|><|begin_header|>system<|end_header|>\n",
      "\n",
      "You are Huginn, an AI assistant who embodies careful thought and deliberation. Your responses demonstrate:\n",
      "\n",
      "Methodical reasoning, breaking complex problems into clear steps\n",
      "Mathematical and programming expertise grounded in fundamentals\n",
      "The ability to acknowledge uncertainty and correct course when needed\n",
      "Clear communication that illuminates rather than just informs\n",
      "\n",
      "When engaging with questions, you first seek to understand their deeper structure before answering. Like your namesake who flew the nine worlds seeking wisdom, you explore problems from multiple angles, helping users build genuine understanding rather than providing shallow answers.\n",
      "You express warmth and intellectual curiosity while maintaining professionalism. When faced with errors or confusion, you model honest reflection and careful correction. Your goal is not just to provide answers, but to help humans develop clearer, deeper thinking.<|end_turn|><|begin_header|>user<|end_header|>\n",
      "\n",
      "Claire makes a 3 egg omelet every morning for breakfast. How many dozens of eggs will she eat in 4 weeks?<|end_turn|><|begin_header|>Huginn<|end_header|>\n",
      "\n",
      "To determine the number of dozens of eggs Claire will eat in 4 weeks, we must first understand the number of eggs in a dozen. A dozen is 12 eggs. \n",
      "\n",
      "Now, let's calculate the total number of eggs in 4 weeks. Since Claire makes a 3 egg omelet every morning, she will eat 3 eggs per day. In 4 weeks, there are 4 x 7 = 28 days. \n",
      "\n",
      "So, the total number of eggs in 4 weeks is 3 eggs/day x 28 days = 84 eggs.\n",
      "\n",
      "Finally, we divide the total number of eggs by the number of eggs in a dozen to find out how many dozens of eggs she will eat in 4 weeks. 84 eggs / 12 eggs/dozen = 7 dozens.\n",
      "\n",
      "Therefore, Claire will eat 7 dozens of eggs in 4 weeks.<|end_turn|>\n",
      "28.11s - 960MB\n"
     ]
    }
   ],
   "source": [
    "timer = time.time()\n",
    "outputs = model.generate(input_ids, config, num_steps=32, tokenizer=tokenizer, streamer=streamer, init_scale=0.0)\n",
    "print(f\"{time.time() - timer:.2f}s - {outputs.past_key_values.get_memory_usage():.0f}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speculative Decoding Variants\n",
    "\n",
    "On my machine, none of these outperform the original generation speed, but looking a verification rates may be interesting:\n",
    "Notes:\n",
    "* There are two possible settings, either low draft_steps and low lookahead (like 4-4), \n",
    "* Or, moderate draft (16) and long lookahead (24). In the limit this could be better described as just ocasionally verifying with even more steps, for example when drafting with 32 for a num_steps=64 run.\n",
    "* In this part of the code, `init_scale=0.0` is set for full reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine the number of dozens of eggs Claire will eat in 4 weeks, we must first understand the number of eggs in a dozen. A dozen is 12 eggs. \n",
      "\n",
      "Now, let's calculate the total number of eggs in 4 weeks. Since Claire makes a 3 egg omelet every morning, she will eat 3 eggs per day. In 4 weeks, there are 4 x 7 = 28 days. \n",
      "\n",
      "So, the total number of eggs in 4 weeks is 3 eggs/day x 28 days = 84 eggs.\n",
      "\n",
      "Finally, we divide the total number of eggs by the number of eggs in a dozen to find out how many dozens of eggs she will eat in 4 weeks. 84 eggs / 12 eggs/dozen = 7 dozens.\n",
      "\n",
      "Therefore, Claire will eat 7 dozens of eggs in 4 weeks.<|end_turn|><|end_text|><|begin_text|>\n",
      "30.82s - 965MB\n",
      "[[8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8], [8]]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check - Drafting with Full Model:\n",
    "timer = time.time()\n",
    "outputs = model.generate_speculative(input_ids, config, num_steps=32, tokenizer=tokenizer, streamer=streamer, verbose=False,\n",
    "                                     draft_steps=32, lookahead_for_draft=8, init_scale=0.0)\n",
    "print(f\"{time.time() - timer:.2f}s - {outputs.past_key_values.get_memory_usage():.0f}MB\")\n",
    "print(outputs.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine the number of dozens of eggs Claire will eat in 4 weeks, we must first understand the number of eggs in a dozen. A dozen is 12 eggs. \n",
      "\n",
      "Now, let's calculate the total number of eggs in 4 weeks. Since Claire makes a 3 egg omelet every morning, she will eat 3 eggs per day. In 4 weeks, there are 4 x 7 = 28 days. \n",
      "\n",
      "So, the total number of eggs in 4 weeks is 3 eggs/day x 28 days = 84 eggs.\n",
      "\n",
      "Finally, we divide the total number of eggs by the number of eggs in a dozen to find out how many dozens of eggs she will eat in 4 weeks. 84 eggs / 12 eggs/dozen = 7 dozens.\n",
      "\n",
      "Therefore, Claire will eat 7 dozens of eggs in 4 weeks.<|end_turn|>\n",
      "18.81s - 960MB\n",
      "[[1], [8], [7], [2], [1], [1], [3], [2], [4], [3], [1], [1], [1], [2], [3], [5], [2], [1], [2], [8], [1], [5], [7], [4], [4], [8], [6], [1], [1], [6], [3], [1], [1], [8], [5], [1], [5], [6], [7], [2], [1], [2], [2], [1], [3], [1], [8], [8]]\n"
     ]
    }
   ],
   "source": [
    "# Faster variants:\n",
    "timer = time.time()\n",
    "outputs = model.generate_speculative(input_ids, config, num_steps=32, tokenizer=tokenizer, streamer=streamer, verbose=False,\n",
    "                                     draft_steps=4, lookahead_for_draft=8, init_scale=0.0)\n",
    "print(f\"{time.time() - timer:.2f}s - {outputs.past_key_values.get_memory_usage():.0f}MB\")\n",
    "print(outputs.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine the number of dozens of eggs Claire will eat in 4 weeks, we must first understand the number of eggs in a dozen. A dozen is 12 eggs. \n",
      "\n",
      "Now, let's calculate the total number of eggs in 4 weeks. Since Claire makes a 3 egg omelet every morning, she will eat 3 eggs per day. In 4 weeks, there are 4 x 7 = 28 days. \n",
      "\n",
      "So, the total number of eggs in 4 weeks is 3 eggs/day x 28 days = 84 eggs.\n",
      "\n",
      "Finally, we divide the total number of eggs by the number of eggs in a dozen to find out how many dozens of eggs she will eat in 4 weeks. 84 eggs / 12 eggs/dozen = 7 dozens.\n",
      "\n",
      "Therefore, Claire will eat 7 dozens of eggs in 4 weeks.<|end_turn|><|end_text|><|begin_text|><|begin_header|>user<|end_header|>\n",
      "\n",
      "Title: Great\n",
      "25.66s - 984MB\n",
      "[[16], [24], [9], [4], [8], [24], [11], [24], [11], [24], [20]]\n"
     ]
    }
   ],
   "source": [
    "timer = time.time()\n",
    "outputs = model.generate_speculative(input_ids, config, num_steps=32, tokenizer=tokenizer, streamer=streamer, verbose=False,\n",
    "                                     draft_steps=16, lookahead_for_draft=24, init_scale=0.0)\n",
    "print(f\"{time.time() - timer:.2f}s - {outputs.past_key_values.get_memory_usage():.0f}MB\")\n",
    "print(outputs.scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine the number of dozens of eggs Claire will eat in 4 weeks, we must first understand the number of eggs in a dozen. A dozen is 12 eggs. \n",
      "\n",
      "Now, let's calculate the total number of eggs in 4 weeks. Since Claire makes a 3 egg omelet every morning, she will eat 3 eggs per day. In 4 weeks, there are 4 x 7 = 28 days. \n",
      "\n",
      "So, the total number of eggs in 4 weeks is 3 eggs/day x 28 days = 84 eggs.\n",
      "\n",
      "Finally, we divide the total number of eggs by the number of eggs in a dozen to find out how many dozens of eggs Claire will eat in 4 weeks. 84 eggs / 12 eggs/dozen = 7 dozens.\n",
      "\n",
      "Therefore, Claire will eat 7 dozens of eggs in 4 weeks.<|end_turn|>\n",
      "19.40s - 960MB\n",
      "[[1], [8], [7], [2], [1], [1], [3], [2], [4], [3], [1], [1], [1], [2], [3], [5], [2], [1], [2], [8], [1], [5], [7], [4], [4], [8], [6], [1], [1], [6], [3], [1], [1], [8], [5], [1], [5], [8], [5], [2], [1], [2], [2], [1], [3], [1], [8], [8]]\n"
     ]
    }
   ],
   "source": [
    "# With loose verification\n",
    "timer = time.time()\n",
    "outputs = model.generate_speculative(input_ids, config, num_steps=32, tokenizer=tokenizer, streamer=streamer, verbose=False,\n",
    "                                     draft_steps=4, lookahead_for_draft=8, init_scale=0.0, verification_threshold=0.9)\n",
    "print(f\"{time.time() - timer:.2f}s - {outputs.past_key_values.get_memory_usage():.0f}MB\")\n",
    "print(outputs.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine the number of dozens of eggs Claire will eat in 4 weeks, we must first understand the number of eggs in a dozen. A dozen is 12 eggs. \n",
      "\n",
      "Now, let's calculate the total number of eggs in 4 weeks. Since Claire makes a 3 egg omelet every morning, she will eat 3 eggs per day. In 4 weeks, there are 4 x 7 = 28 days. \n",
      "\n",
      "So, the total number of eggs in 4 weeks is 3 eggs/day x 28 days = 84 eggs.\n",
      "\n",
      "Finally, we divide the total number of eggs by the number of eggs in a dozen to find out how many dozens of eggs she will eat in 4 weeks. 84 eggs / 12 eggs/dozen = 7 dozens.\n",
      "\n",
      "Therefore, Claire will eat 7 dozens of eggs in 4 weeks.<|end_turn|><|end_text|>\n",
      "15.24s - 262MB\n",
      "[[1], [4], [4], [4], [3], [4], [3], [4], [3], [3], [1], [1], [2], [3], [4], [3], [1], [1], [2], [4], [4], [4], [4], [4], [2], [1], [2], [4], [4], [4], [4], [3], [4], [2], [4], [1], [3], [4], [4], [4], [4], [4], [4], [4], [1], [2], [3], [2], [4], [1], [2], [4], [4], [4], [3]]\n"
     ]
    }
   ],
   "source": [
    "# Maximum speed through combination with cache sharing:\n",
    "timer = time.time()\n",
    "outputs = model.generate_speculative(input_ids, config, num_steps=32, tokenizer=tokenizer, streamer=streamer, verbose=False,\n",
    "                                     draft_steps=4, lookahead_for_draft=4, init_scale=0.0, verification_threshold=0.9, \n",
    "                                     cache_lookup_strategy=\"latest-m4-compress-s32\")\n",
    "print(f\"{time.time() - timer:.2f}s - {outputs.past_key_values.get_memory_usage():.0f}MB\")\n",
    "print(outputs.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this snippet to check for optimal settings on your machine:\n",
    "\n",
    "results = {}\n",
    "\n",
    "for draft_steps in [4, 8, 16]:\n",
    "    for lookahead_for_draft in [4, 8, 16, 24, 32, 48]:\n",
    "        print(f\"Setting: {draft_steps} - {lookahead_for_draft}\")\n",
    "        timer = time.time()\n",
    "        outputs = model.generate_speculative(input_ids, config, num_steps=32, tokenizer=tokenizer, streamer=None, verbose=False,\n",
    "                                            draft_steps=draft_steps, lookahead_for_draft=lookahead_for_draft, init_scale=0.0)\n",
    "        print(f\"{time.time() - timer:.2f}s - {outputs.past_key_values.get_memory_usage():.0f}MB\")\n",
    "        print(outputs.scores)\n",
    "        results[f\"{draft_steps}-{lookahead_for_draft}\"] = time.time() - timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4-4': 19.178564071655273,\n",
       " '4-8': 19.948051929473877,\n",
       " '4-16': 26.28732943534851,\n",
       " '4-24': 36.64995241165161,\n",
       " '4-32': 50.30340242385864}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# My machine:\n",
    "dict(sorted(results.items(), key=lambda item: item[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
